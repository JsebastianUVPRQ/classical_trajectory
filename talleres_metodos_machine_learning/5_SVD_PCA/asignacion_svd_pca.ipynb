{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7c9fcbdb9aa04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a pseudo code for vectorize photos of zeros and ones\n",
    "# use SVD and PCA\n",
    "# do the algebra logic by \"hand\", dont use scikit-learn\n"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Importamos las librer铆as necesarias\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "81de9b1a941bb785",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0d2515b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\n",
    "\n",
    "# Funci贸n para cargar, procesar y vectorizar im谩genes\n",
    "def cargar_procesar_imagenes(directorio, nuevo_tam=(50, 50)):\n",
    "    imagenes_vectorizadas = []\n",
    "    \n",
    "    for archivo in os.listdir(directorio):\n",
    "        if archivo.endswith(\".jpeg\"):\n",
    "            # Cargar imagen en escala de grises\n",
    "            ruta_imagen = os.path.join(directorio, archivo)\n",
    "            img = cv2.imread(ruta_imagen, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                # Reducir el tama帽o de la imagen\n",
    "                img_redimensionada = cv2.resize(img, nuevo_tam)\n",
    "                # Aumentar contraste usando equalizaci贸n de histograma\n",
    "                img_contraste = cv2.equalizeHist(img_redimensionada)\n",
    "                # Vectorizar la imagen (convertir la matriz en un vector)\n",
    "                imagenes_vectorizadas.append(img_contraste.flatten())\n",
    "    \n",
    "    return np.array(imagenes_vectorizadas)\n",
    "\n",
    "# 1. Cargar y procesar las im谩genes del directorio './fotos/'\n",
    "directorio = \"./fotos/\"\n",
    "X = cargar_procesar_imagenes(directorio)\n",
    "\n",
    "# Verificamos que se hayan cargado im谩genes\n",
    "if X.shape[0] == 0:\n",
    "    print(\"No se encontraron im谩genes .jpeg en el directorio especificado.\")\n",
    "else:\n",
    "    print(f\"Se han cargado {X.shape[0]} im谩genes con {X.shape[1]} caracter铆sticas cada una.\")\n",
    "\n",
    "# 2. Estandarizaci贸n: centramos los datos (restamos la media de cada caracter铆stica)\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_centered = X - X_mean / 255.0\n",
    "\n",
    "# 3. No usar la matriz de covarianza, sino la matriz de productos internos\n",
    "XtX = X_centered.T @ X_centered\n",
    "\n",
    "# 4. Calcular los vectores y valores propios de la matriz de productos internos\n",
    "eigvals, eigvecs = np.linalg.eig(XtX)\n",
    "\n",
    "\n",
    "# 5. Ordenar los valores propios y vectores en orden descendente\n",
    "indices_orden = np.argsort(eigvals)[::-1]\n",
    "eigvals = eigvals[indices_orden]\n",
    "eigvecs = eigvecs[:, indices_orden]\n",
    "\n",
    "# 6. Seleccionar los 2 primeros componentes principales para visualizaci贸n\n",
    "PCs = eigvecs[:, :2]\n",
    "\n",
    "# 7. Proyecci贸n de los datos en el espacio de los componentes principales\n",
    "X_pca = X_centered @ PCs\n",
    "\n",
    "# 8. Clasificaci贸n simple: '1' si PC1 > media, '0' en caso contrario\n",
    "umbral = np.mean(X_pca[:, 0])\n",
    "etiquetas = (X_pca[:, 0] > umbral).astype(int)\n",
    "\n",
    "# 8. Visualizaci贸n: scatter plot de las proyecciones PCA\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=etiquetas, cmap='bwr', alpha=0.7)\n",
    "plt.xlabel(\"Primer Componente Principal (PC1)\")\n",
    "plt.ylabel(\"Segundo Componente Principal (PC2)\")\n",
    "plt.title(\"Distribuci贸n de im谩genes en el espacio PCA\\n(rojo=1, azul=0)\")\n",
    "plt.colorbar(scatter, label=\"Etiqueta\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 9. Mostrar la cantidad de im谩genes clasificadas como '1' y '0'\n",
    "num_1 = np.sum(etiquetas)\n",
    "num_0 = len(etiquetas) - num_1\n",
    "print(f\"Im谩genes clasificadas como '1': {num_1}\")\n",
    "print(f\"Im谩genes clasificadas como '0': {num_0}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918480ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63daa731",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Introducci贸n y Teor铆a\n",
    "# \n",
    "# **Objetivo**: Clasificar im谩genes de d铆gitos '0' y '1' mediante t茅cnicas de 谩lgebra lineal (SVD y PCA), reduciendo la dimensionalidad de los datos y visualizando patrones.\n",
    "# \n",
    "# ### Fundamentos Matem谩ticos\n",
    "# 1. **Vectorizaci贸n de Im谩genes**:\n",
    "#   - Una imagen en escala de grises se representa como matriz 2D de pixeles (valores 0-255).\n",
    "#   - Para an谩lisis num茅rico, se \"aplana\" a un vector 1D de tama帽o `ancho  alto`.\n",
    "# \n",
    "# 2. **Descomposici贸n en Valores Singulares (SVD)**:\n",
    "#   - Cualquier matriz \\( X \\) puede descomponerse como:\n",
    "#     \\[\n",
    "#     X = U \\Sigma V^T\n",
    "#     \\]\n",
    "#     donde \\( V^T \\) contiene los vectores singulares (direcciones de m谩xima varianza).\n",
    "# \n",
    "# 3. **An谩lisis de Componentes Principales (PCA)**:\n",
    "#   - M茅todo estad铆stico para proyectar datos en direcciones ortogonales de m谩xima varianza.\n",
    "#   - Los componentes principales son los autovectores de la matriz de covarianza \\( X^TX \\).\n",
    "# \n",
    "# ### Relaci贸n SVD-PCA\n",
    "# - Las columnas de \\( V \\) en SVD corresponden a los componentes principales.\n",
    "# - Los valores singulares \\( \\Sigma \\) se relacionan con la varianza explicada: \\( \\lambda_i = \\sigma_i^2/(n-1) \\).\n",
    "\n",
    "# %%\n",
    "# Importaci贸n de librer铆as\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Desarrollo y Pormenores (Procesamiento de Im谩genes)\n",
    "# \n",
    "# ### Etapa 1: Preprocesamiento\n",
    "# **Prop贸sito**: Normalizar caracter铆sticas para an谩lisis efectivo.\n",
    "# 1. **Escala de Grises**: Elimina informaci贸n redundante de color.\n",
    "# 2. **Reducci贸n de Tama帽o (75x75)**:\n",
    "#   - Reduce dimensionalidad computacional.\n",
    "#   - Elimina ruido y bordes irrelevantes.\n",
    "# 3. **Aumento de Contraste**:\n",
    "#   - Equalizaci贸n de histograma redistribuye intensidades de pixeles.\n",
    "#   - Mejora la separabilidad entre d铆gitos y fondo.\n",
    "\n",
    "# %%\n",
    "def cargar_procesar_imagenes(directorio, nuevo_tam=(50, 50)):\n",
    "    imagenes_vectorizadas = []\n",
    "    for archivo in os.listdir(directorio):\n",
    "        if archivo.endswith(\".jpeg\"):\n",
    "            img = cv2.imread(os.path.join(directorio, archivo), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                img_redim = cv2.resize(img, nuevo_tam)  # Uniformiza dimensiones\n",
    "                img_eq = cv2.equalizeHist(img_redim)     # Mejora contraste\n",
    "                imagenes_vectorizadas.append(img_eq.flatten())  # Vectorizaci贸n\n",
    "    return np.array(imagenes_vectorizadas)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Etapa 2: Estandarizaci贸n de Datos\n",
    "# **Por qu茅 centrar?**:\n",
    "# - PCA es sensible a la media de los datos.\n",
    "# - Restar la media garantiza que el primer componente principal capture la direcci贸n de m谩xima varianza verdadera, no artefactos por desplazamiento.\n",
    "\n",
    "# %%\n",
    "X_mean = np.mean(X, axis=0)        # Calcula media por pixel\n",
    "X_centered = X - X_mean/255.0      # Centrado escalado (evita overflow num茅rico)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Etapa 3: C谩lculo de Componentes Principales\n",
    "# **Matriz de Covarianza** \\( X^TX \\):\n",
    "# - Captura correlaciones entre pixeles.\n",
    "# - Autovectores = Direcciones principales de variaci贸n.\n",
    "# - Autovalores = Magnitud de varianza en cada direcci贸n.\n",
    "\n",
    "# %%\n",
    "XtX = X_centered.T @ X_centered           # Matriz de covarianza (producto interno)\n",
    "eigvals, eigvecs = np.linalg.eig(XtX)     # Descomposici贸n espectral\n",
    "\n",
    "# Orden descendente por varianza explicada\n",
    "indices_orden = np.argsort(eigvals)[::-1]\n",
    "eigvals = eigvals[indices_orden]          # Autovalores ordenados\n",
    "eigvecs = eigvecs[:, indices_orden]       # Autovectores ordenados\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Resultados y Visualizaci贸n\n",
    "# \n",
    "# ### Proyecci贸n en 2D\n",
    "# - **PC1 y PC2** capturan las direcciones de m谩xima y segunda m谩xima varianza.\n",
    "# - La separaci贸n en el gr谩fico sugiere que PC1 diferencia entre clases.\n",
    "\n",
    "# %%\n",
    "PCs = eigvecs[:, :2]                # Selecciona 2 primeros componentes\n",
    "X_pca = X_centered @ PCs            # Proyecci贸n lineal\n",
    "\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=etiquetas, cmap='bwr')\n",
    "plt.xlabel('PC1 (Varianza: {:.1f}%)'.format(100*eigvals[0]/np.sum(eigvals)))\n",
    "plt.ylabel('PC2 (Varianza: {:.1f}%)'.format(100*eigvals[1]/np.sum(eigvals)))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Conclusiones\n",
    "# \n",
    "# 1. **Efectividad de PCA**:\n",
    "#   - Reducci贸n de 5,625 caracter铆sticas (75x75) a solo 2 manteniendo estructura.\n",
    "#   - Separaci贸n visual sugiere que los d铆gitos tienen patrones espaciales distintivos.\n",
    "# \n",
    "# 2. **Limitaciones**:\n",
    "#   - Clasificaci贸n simple por umbral podr铆a fallar con datos no lineales.\n",
    "#   - Sensible a rotaciones o traslaciones en im谩genes.\n",
    "# \n",
    "# 3. **Extensiones Futuras**:\n",
    "#   - Usar SVD truncado para compresi贸n antes de PCA.\n",
    "#   - Implementar clasificador SVM en el espacio PCA.\n",
    "# \n",
    "# **Aplicaciones**: Reconocimiento de caracteres manuscritos, detecci贸n de anomal铆as en im谩genes m茅dicas.\n",
    "\n",
    "# %%\n",
    "# Resultados de clasificaci贸n\n",
    "print(f\"Clasificados como '1': {num_1} | Clasificados como '0': {num_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f81d7",
   "metadata": {},
   "source": [
    "% \\newpage\n",
    "\n",
    "% \\vfill\n",
    "\\bibliographystyle{siam}\n",
    "\\bibliography{referencias}\n",
    "\n",
    "\\begin{thebibliography}{1}\n",
    "\\bibitem{Golub} Golub, G. H., \\& Van Loan, C. F. (2013). Matrix computations (Vol. 3). JHU Press.\n",
    "\\bibitem{McKinney} **McKinney, W.** (2017). *Python for Data Analysis* (2nd ed.). OReilly Media. ISBN: 978-1491957660\n",
    "\n",
    "\\end{thebibliography}\n",
    "\n",
    "ejemplo de referencia: \\cite{   } y \\cite{  }.\n",
    "\n",
    "驴Para qu茅 sirve \\vfill? Esto se utiliza para llenar el espacio restante de la p谩gina con espacios en blanco. Se puede utilizar para centrar contenido "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080398c",
   "metadata": {},
   "source": [
    "# Si aparece dos veces el titulo \"referencias\" se puede solucionar con el siguiente comando \n",
    "\\renewcommand{\\refname}{}\n",
    "\\renewcommand{\\refname}{Referencias}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ea308",
   "metadata": {},
   "source": [
    "(Due to technical issues, the search service is temporarily unavailable.)\n",
    "\n",
    "Heres a **bibliography** tailored to the theoretical foundations of SVD, PCA, and image analysis discussed in your report. These references include textbooks, research papers, and authoritative resources for mathematical and applied contexts:\n",
    "\n",
    "---\n",
    "\n",
    "### **Books**\n",
    "1. **Strang, G.** (2006). *Linear Algebra and Its Applications* (4th ed.).  \n",
    "   - *Relevance*: Classic textbook explaining the theory of Singular Value Decomposition (SVD) and its geometric interpretation (Chapter 6).  \n",
    "   - ISBN: 978-0030105678\n",
    "\n",
    "2. **Jolliffe, I. T.** (2002). *Principal Component Analysis* (2nd ed.). Springer.  \n",
    "   - *Relevance*: Definitive guide to PCA, including proofs, applications, and connections to covariance matrices.  \n",
    "   - ISBN: 978-0387954424\n",
    "\n",
    "3. **Bishop, C. M.** (2006). *Pattern Recognition and Machine Learning*. Springer.  \n",
    "   - *Relevance*: Chapter 12 covers PCA in the context of dimensionality reduction and latent variable models.  \n",
    "   - ISBN: 978-0387310732\n",
    "\n",
    "4. **Golub, G. H., & Van Loan, C. F.** (2013). *Matrix Computations* (4th ed.). Johns Hopkins University Press.  \n",
    "   - *Relevance*: Detailed algorithms for SVD and eigenvalue decomposition (Chapters 2 and 8).  \n",
    "   - ISBN: 978-1421407944\n",
    "\n",
    "---\n",
    "\n",
    "### **Research Papers**\n",
    "5. **Turk, M., & Pentland, A.** (1991). *Eigenfaces for Recognition*. Journal of Cognitive Neuroscience.  \n",
    "   - *Relevance*: Seminal paper applying PCA (called eigenfaces) to facial recognition.  \n",
    "   - DOI: [10.1162/jocn.1991.3.1.71](https://doi.org/10.1162/jocn.1991.3.1.71)\n",
    "\n",
    "6. **Shlens, J.** (2014). *A Tutorial on Principal Component Analysis*. arXiv.  \n",
    "   - *Relevance*: Intuitive derivation of PCA with geometric interpretations.  \n",
    "   - arXiv: [1404.1100](https://arxiv.org/abs/1404.1100)\n",
    "\n",
    "---\n",
    "\n",
    "### **Applied Resources**\n",
    "7. **McKinney, W.** (2017). *Python for Data Analysis* (2nd ed.). OReilly Media.  \n",
    "   - *Relevance*: Practical implementation of PCA/SVD using NumPy and SciPy (Chapter 13).  \n",
    "   - ISBN: 978-1491957660\n",
    "\n",
    "8. **VanderPlas, J.** (2016). *Python Data Science Handbook*. OReilly Media.  \n",
    "   - *Relevance*: Hands-on examples of PCA for image compression and visualization.  \n",
    "   - Free access: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\n",
    "\n",
    "9. **OpenCV Documentation** (2023). *Image Processing and Computer Vision*.  \n",
    "   - *Relevance*: Official docs for image preprocessing steps (e.g., histogram equalization, resizing).  \n",
    "   - Link: [OpenCV Documentation](https://docs.opencv.org/)\n",
    "\n",
    "---\n",
    "\n",
    "### **Online Tutorials and Tools**\n",
    "10. **NumPy Documentation** (2023). *numpy.linalg.svd*.  \n",
    "    - *Relevance*: Official guide to SVD implementation in NumPy.  \n",
    "    - Link: [numpy.linalg.svd](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
    "\n",
    "11. **Scikit-learn Documentation** (2023). *Decomposing signals in components (matrix factorization problems)*.  \n",
    "    - *Relevance*: Theory and code for PCA, though your report avoids using scikit-learn.  \n",
    "    - Link: [Scikit-learn Decomposition](https://scikit-learn.org/stable/modules/decomposition.html)\n",
    "\n",
    "---\n",
    "\n",
    "### **Specialized Topics**\n",
    "12. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning*. MIT Press.  \n",
    "    - *Relevance*: Chapter 2 (Linear Algebra) and Chapter 5 (Machine Learning Basics) contextualize PCA in modern ML.  \n",
    "    - Free access: [Deep Learning Book](https://www.deeplearningbook.org/)\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Cite**\n",
    "For academic reports, use APA/MLA/IEEE formats depending on requirements. Example (APA):  \n",
    "> Strang, G. (2006). *Linear Algebra and Its Applications* (4th ed.). Brooks Cole.\n",
    "\n",
    "Let me know if you need help formatting citations for a specific style! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svdpca.venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
